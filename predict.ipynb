{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from random import random, sample, seed\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\n",
    "from keras.layers.core import Masking, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem_usage():                                                                                                                               \n",
    "    process = psutil.Process(os.getpid())                                                                                                          \n",
    "    return process.memory_info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmem(rss=253263872, vms=1253588992, shared=102899712, text=2322432, lib=0, data=215437312, dirty=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/submissions.csv'\n",
    "embeddings_path = 'data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "hours = []\n",
    "minutes = []\n",
    "dayofweeks = []\n",
    "dayofyears = []\n",
    "is_top_submission = []\n",
    "\n",
    "# max_rows = 16000 # my crappy computer can't handle too many rows\n",
    "\n",
    "with open(data_path, 'r', encoding=\"latin1\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    i = 0\n",
    "    for submission in reader:\n",
    "#         if i >= max_rows:\n",
    "#             break\n",
    "        i += 1\n",
    "        titles.append(submission['title'])\n",
    "        hours.append(submission['hour'])\n",
    "        minutes.append(submission['minute'])\n",
    "        dayofweeks.append(submission['dayofweek'])\n",
    "        dayofyears.append(submission['dayofyear'])\n",
    "        is_top_submission.append(submission['is_top_submission'])\n",
    "            \n",
    "titles = np.array(titles)\n",
    "hours = np.array(hours, dtype=int)\n",
    "minutes = np.array(minutes, dtype=int)\n",
    "dayofweeks = np.array(dayofweeks, dtype=int)\n",
    "dayofyears = np.array(dayofyears, dtype=int)\n",
    "is_top_submission = np.array(is_top_submission, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People who downloaded their Google data and went through it, what were the most unsettling things you found out they had stored about you?'\n",
      " \"Have you ever felt you don't know/have forgotten who you really are? That you've spent years just adapting to surroundings to make life easier and don't know what's the real you anymore? If so, how did you overcome this?\"]\n",
      "(1397703,)\n",
      "[22  3]\n",
      "[45 53]\n",
      "[3 4]\n",
      "[219 206]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(titles[0:2])\n",
    "print(titles.shape)\n",
    "print(hours[0:2])\n",
    "print(minutes[0:2])\n",
    "print(dayofweeks[0:2])\n",
    "print(dayofyears[0:2])\n",
    "print(is_top_submission[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742076106297261"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(is_top_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('people', 148888), ('who', 120348), ('downloaded', 187), ('their', 35401), ('google', \n",
      "{'you': 1, 'what': 2, 'the': 3, 'to': 4, 'a': 5, 'of': 6, 'your': 7, 'is': 8, 'do': 9, 'and': 10, 'i\n",
      "127351\n"
     ]
    }
   ],
   "source": [
    "max_features = 40000\n",
    "\n",
    "word_tokenizer = Tokenizer(max_features)\n",
    "word_tokenizer.fit_on_texts(titles)\n",
    "\n",
    "print(str(word_tokenizer.word_counts)[0:100])\n",
    "print(str(word_tokenizer.word_index)[0:100])\n",
    "print(len(word_tokenizer.word_counts))   # true word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 26, 5020, 73, 736, 1626, 10, 392, 224, 15, 2, 54, 3, 30, 3319, 95, 1, 187, 59, 62, 47, 6354, 35, 1]\n"
     ]
    }
   ],
   "source": [
    "titles_tf = word_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 736 1626   10  392  224   15    2   54    3   30 3319   95    1  187\n",
      "   59   62   47 6354   35    1]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "titles_tf = sequence.pad_sequences(titles_tf, maxlen=maxlen)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0919e-03  3.3324e-01  3.5743e-01 -5.4041e-01  8.2032e-01 -4.9391e-01\n",
      " -3.2588e-01  1.9972e-03 -2.3829e-01  3.5554e-01 -6.0655e-01  9.8932e-01\n",
      " -2.1786e-01  1.1236e-01  1.1494e+00  7.3284e-01  5.1182e-01  2.9287e-01\n",
      "  2.8388e-01 -1.3590e+00 -3.7951e-01  5.0943e-01  7.0710e-01  6.2941e-01\n",
      "  1.0534e+00 -2.1756e+00 -1.3204e+00  4.0001e-01  1.5741e+00 -1.6600e+00\n",
      "  3.7721e+00  8.6949e-01 -8.0439e-01  1.8390e-01 -3.4332e-01  1.0714e-02\n",
      "  2.3969e-01  6.6748e-02  7.0117e-01 -7.3702e-01  2.0877e-01  1.1564e-01\n",
      " -1.5190e-01  8.5908e-01  2.2620e-01  1.6519e-01  3.6309e-01 -4.5697e-01\n",
      " -4.8969e-02  1.1316e+00]\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = {}\n",
    "\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec\n",
    "        \n",
    "print(embedding_vectors['you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00]\n",
      " [-1.0919e-03  3.3324e-01  3.5743e-01 -5.4041e-01  8.2032e-01 -4.9391e-01\n",
      "  -3.2588e-01  1.9972e-03 -2.3829e-01  3.5554e-01 -6.0655e-01  9.8932e-01\n",
      "  -2.1786e-01  1.1236e-01  1.1494e+00  7.3284e-01  5.1182e-01  2.9287e-01\n",
      "   2.8388e-01 -1.3590e+00 -3.7951e-01  5.0943e-01  7.0710e-01  6.2941e-01\n",
      "   1.0534e+00 -2.1756e+00 -1.3204e+00  4.0001e-01  1.5741e+00 -1.6600e+00\n",
      "   3.7721e+00  8.6949e-01 -8.0439e-01  1.8390e-01 -3.4332e-01  1.0714e-02\n",
      "   2.3969e-01  6.6748e-02  7.0117e-01 -7.3702e-01  2.0877e-01  1.1564e-01\n",
      "  -1.5190e-01  8.5908e-01  2.2620e-01  1.6519e-01  3.6309e-01 -4.5697e-01\n",
      "  -4.8969e-02  1.1316e+00]]\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((max_features + 1, 50))\n",
    "\n",
    "for word, i in word_tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if embedding_vector is not None and i <= max_features:\n",
    "        weights_matrix[i] = embedding_vector\n",
    "\n",
    "# index 0 vector should be all zeroes, index 1 vector should be the same one as above\n",
    "print(weights_matrix[0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[218 205 220 209 234 190 233 224 206 151]\n"
     ]
    }
   ],
   "source": [
    "dayofyears_tf = dayofyears - 1\n",
    "\n",
    "print(dayofyears_tf[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 19:35:53.833239 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0922 19:35:53.834784 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0922 19:35:53.900544 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 19:35:53.906965 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0922 19:35:53.918952 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles_input = Input(shape=(maxlen,), name='titles_input')\n",
    "titles_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(titles_input)\n",
    "titles_pooling = GlobalAveragePooling1D()(titles_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_output = Dense(1, activation='sigmoid', name='aux_out')(titles_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embedding_dims = 64\n",
    "\n",
    "hours_input = Input(shape=(1,), name='hours_input')\n",
    "hours_embedding = Embedding(24, meta_embedding_dims)(hours_input)\n",
    "hours_reshape = Reshape((meta_embedding_dims,))(hours_embedding)\n",
    "\n",
    "dayofweeks_input = Input(shape=(1,), name='dayofweeks_input')\n",
    "dayofweeks_embedding = Embedding(7, meta_embedding_dims)(dayofweeks_input)\n",
    "dayofweeks_reshape = Reshape((meta_embedding_dims,))(dayofweeks_embedding)\n",
    "\n",
    "minutes_input = Input(shape=(1,), name='minutes_input')\n",
    "minutes_embedding = Embedding(60, meta_embedding_dims)(minutes_input)\n",
    "minutes_reshape = Reshape((meta_embedding_dims,))(minutes_embedding)\n",
    "\n",
    "dayofyears_input = Input(shape=(1,), name='dayofyears_input')\n",
    "dayofyears_embedding = Embedding(366, meta_embedding_dims)(dayofyears_input)\n",
    "dayofyears_reshape = Reshape((meta_embedding_dims,))(dayofyears_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = concatenate([titles_pooling, hours_reshape, dayofweeks_reshape, minutes_reshape, dayofyears_reshape])\n",
    "\n",
    "hidden_1 = Dense(256, activation='relu')(merged)\n",
    "hidden_1 = BatchNormalization()(hidden_1)\n",
    "\n",
    "main_output = Dense(1, activation='sigmoid', name='main_out')(hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 19:35:54.870394 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0922 19:35:54.897211 140623294682944 deprecation.py:323] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "titles_input (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hours_input (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dayofweeks_input (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "minutes_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dayofyears_input (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 50)       2000050     titles_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 64)        1536        hours_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 64)        448         dayofweeks_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 64)        3840        minutes_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 64)        23424       dayofyears_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 50)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 64)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 64)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 64)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 306)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          78592       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_out (Dense)                (None, 1)            257         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "aux_out (Dense)                 (None, 1)            51          global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,109,222\n",
      "Trainable params: 2,108,710\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[titles_input,\n",
    "                      hours_input,\n",
    "                      dayofweeks_input,\n",
    "                      minutes_input,\n",
    "                      dayofyears_input], outputs=[main_output, aux_output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss_weights=[1, 0.2])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(123)\n",
    "split = 0.2\n",
    "\n",
    "# returns randomized indices with no repeats\n",
    "idx = sample(range(titles_tf.shape[0]), titles_tf.shape[0])\n",
    "\n",
    "titles_tf = titles_tf[idx, :]\n",
    "hours = hours[idx]\n",
    "dayofweeks = dayofweeks[idx]\n",
    "minutes = minutes[idx]\n",
    "dayofyears_tf = dayofyears_tf[idx]\n",
    "is_top_submission = is_top_submission[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6732667954496674\n"
     ]
    }
   ],
   "source": [
    "print(1 - np.mean(is_top_submission[:(int(titles_tf.shape[0] * split))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('training.csv')\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1118162 samples, validate on 279541 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 19:36:05.376952 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0922 19:36:05.378093 140623294682944 deprecation_wrapper.py:119] From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1118162/1118162 [==============================] - 589s 527us/step - loss: 0.6452 - main_out_loss: 0.5311 - aux_out_loss: 0.5702 - main_out_acc: 0.7044 - aux_out_acc: 0.6776 - val_loss: 0.6673 - val_main_out_loss: 0.5518 - val_aux_out_loss: 0.5774 - val_main_out_acc: 0.6889 - val_aux_out_acc: 0.6723\n",
      "Epoch 2/10\n",
      "1118162/1118162 [==============================] - 585s 523us/step - loss: 0.6413 - main_out_loss: 0.5274 - aux_out_loss: 0.5698 - main_out_acc: 0.7080 - aux_out_acc: 0.6778 - val_loss: 0.6723 - val_main_out_loss: 0.5566 - val_aux_out_loss: 0.5781 - val_main_out_acc: 0.6859 - val_aux_out_acc: 0.6724\n",
      "Epoch 3/10\n",
      "1118162/1118162 [==============================] - 586s 524us/step - loss: 0.6383 - main_out_loss: 0.5244 - aux_out_loss: 0.5695 - main_out_acc: 0.7104 - aux_out_acc: 0.6781 - val_loss: 0.6761 - val_main_out_loss: 0.5603 - val_aux_out_loss: 0.5786 - val_main_out_acc: 0.6836 - val_aux_out_acc: 0.6704\n",
      "Epoch 4/10\n",
      "1118162/1118162 [==============================] - 584s 522us/step - loss: 0.6352 - main_out_loss: 0.5213 - aux_out_loss: 0.5692 - main_out_acc: 0.7139 - aux_out_acc: 0.6784 - val_loss: 0.6773 - val_main_out_loss: 0.5614 - val_aux_out_loss: 0.5793 - val_main_out_acc: 0.6811 - val_aux_out_acc: 0.6696\n",
      "Epoch 5/10\n",
      "1118162/1118162 [==============================] - 590s 528us/step - loss: 0.6319 - main_out_loss: 0.5181 - aux_out_loss: 0.5690 - main_out_acc: 0.7164 - aux_out_acc: 0.6787 - val_loss: 0.6827 - val_main_out_loss: 0.5669 - val_aux_out_loss: 0.5794 - val_main_out_acc: 0.6807 - val_aux_out_acc: 0.6710\n",
      "Epoch 6/10\n",
      "1118162/1118162 [==============================] - 587s 525us/step - loss: 0.6291 - main_out_loss: 0.5153 - aux_out_loss: 0.5688 - main_out_acc: 0.7183 - aux_out_acc: 0.6789 - val_loss: 0.6833 - val_main_out_loss: 0.5673 - val_aux_out_loss: 0.5803 - val_main_out_acc: 0.6766 - val_aux_out_acc: 0.6690\n",
      "Epoch 7/10\n",
      "1118162/1118162 [==============================] - 586s 524us/step - loss: 0.6260 - main_out_loss: 0.5123 - aux_out_loss: 0.5687 - main_out_acc: 0.7214 - aux_out_acc: 0.6789 - val_loss: 0.6895 - val_main_out_loss: 0.5734 - val_aux_out_loss: 0.5803 - val_main_out_acc: 0.6765 - val_aux_out_acc: 0.6702\n",
      "Epoch 8/10\n",
      "1118162/1118162 [==============================] - 588s 526us/step - loss: 0.6229 - main_out_loss: 0.5092 - aux_out_loss: 0.5686 - main_out_acc: 0.7237 - aux_out_acc: 0.6793 - val_loss: 0.6890 - val_main_out_loss: 0.5730 - val_aux_out_loss: 0.5802 - val_main_out_acc: 0.6725 - val_aux_out_acc: 0.6685\n",
      "Epoch 9/10\n",
      "1118162/1118162 [==============================] - 589s 527us/step - loss: 0.6199 - main_out_loss: 0.5062 - aux_out_loss: 0.5684 - main_out_acc: 0.7265 - aux_out_acc: 0.6793 - val_loss: 0.6930 - val_main_out_loss: 0.5768 - val_aux_out_loss: 0.5809 - val_main_out_acc: 0.6748 - val_aux_out_acc: 0.6679\n",
      "Epoch 10/10\n",
      "1118162/1118162 [==============================] - 591s 528us/step - loss: 0.6166 - main_out_loss: 0.5030 - aux_out_loss: 0.5683 - main_out_acc: 0.7291 - aux_out_acc: 0.6795 - val_loss: 0.6989 - val_main_out_loss: 0.5827 - val_aux_out_loss: 0.5810 - val_main_out_acc: 0.6666 - val_aux_out_acc: 0.6677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe50aa91990>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([titles_tf, hours, dayofweeks, minutes, dayofyears_tf], [is_top_submission, is_top_submission],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=split, callbacks=[csv_logger, tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, maxlen):\n",
    "    encoded = word_tokenizer.texts_to_sequences([text])\n",
    "    return sequence.pad_sequences(encoded, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.7824525]], dtype=float32), array([[0.51423174]], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would drastically change if you removed a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "input_hour = np.array([15])\n",
    "input_minute = np.array([46])\n",
    "input_dayofweek = np.array([1])\n",
    "input_dayofyear = np.array([16 - 1])\n",
    "\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

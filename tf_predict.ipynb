{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from random import random, sample, seed\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import resource\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\n",
    "from tensorflow.keras.layers import Masking, Dropout, Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "data_path = 'data/submissions.csv'\n",
    "embeddings_path = 'data/glove.6B.50d.txt'\n",
    "maxlen = 20 # max input length\n",
    "batch_size = 32\n",
    "embedding_dims = 50 # word embedding dim\n",
    "meta_embedding_dims = 64 # metadata embedding dim\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "hours = []\n",
    "minutes = []\n",
    "dayofweeks = []\n",
    "dayofyears = []\n",
    "is_top_submission = []\n",
    "\n",
    "max_rows = 16000\n",
    "\n",
    "with open(data_path, 'r', encoding=\"latin1\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    i = 0\n",
    "    for submission in reader:\n",
    "        if i >= max_rows:\n",
    "            break\n",
    "        i += 1\n",
    "        titles.append(submission['title'])\n",
    "        hours.append(submission['hour'])\n",
    "        minutes.append(submission['minute'])\n",
    "        dayofweeks.append(submission['dayofweek'])\n",
    "        dayofyears.append(submission['dayofyear'])\n",
    "        is_top_submission.append(submission['is_top_submission'])\n",
    "            \n",
    "titles = np.array(titles)\n",
    "hours = np.array(hours, dtype=int)\n",
    "minutes = np.array(minutes, dtype=int)\n",
    "dayofweeks = np.array(dayofweeks, dtype=int)\n",
    "dayofyears = np.array(dayofyears, dtype=int)\n",
    "is_top_submission = np.array(is_top_submission, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_rows//2, max_rows):\n",
    "    is_top_submission[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 40000\n",
    "\n",
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\n",
    "word_tokenizer.fit_on_texts(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tf = word_tokenizer.texts_to_sequences(titles)\n",
    "titles_tf = tf.keras.preprocessing.sequence.pad_sequences(titles_tf, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = {}\n",
    "\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((max_features + 1, 50))\n",
    "\n",
    "for word, i in word_tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if embedding_vector is not None and i <= max_features:\n",
    "        weights_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero based year\n",
    "dayofyears_tf = dayofyears - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sess.close()\n",
    "tf.keras.backend.clear_session()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    titles_input = Input(shape=(maxlen,), name='titles_input')\n",
    "#     titles_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(titles_input)\n",
    "#     titles_pooling = GlobalAveragePooling1D()(titles_embedding)\n",
    "    \n",
    "    aux_output = Dense(1, activation='sigmoid', name='aux_out')(titles_input)\n",
    "    \n",
    "#     hours_input = Input(shape=(1,), name='hours_input')\n",
    "#     hours_embedding = Embedding(24, meta_embedding_dims)(hours_input)\n",
    "#     hours_reshape = Reshape((meta_embedding_dims,))(hours_embedding)\n",
    "\n",
    "#     dayofweeks_input = Input(shape=(1,), name='dayofweeks_input')\n",
    "#     dayofweeks_embedding = Embedding(7, meta_embedding_dims)(dayofweeks_input)\n",
    "#     dayofweeks_reshape = Reshape((meta_embedding_dims,))(dayofweeks_embedding)\n",
    "\n",
    "#     minutes_input = Input(shape=(1,), name='minutes_input')\n",
    "#     minutes_embedding = Embedding(60, meta_embedding_dims)(minutes_input)\n",
    "#     minutes_reshape = Reshape((meta_embedding_dims,))(minutes_embedding)\n",
    "\n",
    "#     dayofyears_input = Input(shape=(1,), name='dayofyears_input')\n",
    "#     dayofyears_embedding = Embedding(366, meta_embedding_dims)(dayofyears_input)\n",
    "#     dayofyears_reshape = Reshape((meta_embedding_dims,))(dayofyears_embedding)\n",
    "    \n",
    "#     merged = concatenate([titles_pooling, hours_reshape, dayofweeks_reshape, minutes_reshape, dayofyears_reshape])\n",
    "\n",
    "#     hidden_1 = Dense(256, activation='relu')(merged)\n",
    "#     hidden_1 = BatchNormalization()(hidden_1)\n",
    "\n",
    "#     main_output = Dense(1, activation='sigmoid', name='main_out')(hidden_1)\n",
    "    \n",
    "#     model = Model(inputs=[titles_input,\n",
    "#                       hours_input,\n",
    "#                       dayofweeks_input,\n",
    "#                       minutes_input,\n",
    "#                       dayofyears_input], outputs=[main_output, aux_output])\n",
    "    \n",
    "    model = Model(inputs=[titles_input], outputs=[aux_output])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "titles_input (InputLayer)    [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "aux_out (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(123)\n",
    "split = 0.2\n",
    "\n",
    "# returns randomized indices with no repeats\n",
    "idx = sample(range(titles_tf.shape[0]), titles_tf.shape[0])\n",
    "\n",
    "titles_tf = titles_tf[idx, :]\n",
    "hours = hours[idx]\n",
    "dayofweeks = dayofweeks[idx]\n",
    "minutes = minutes[idx]\n",
    "dayofyears_tf = dayofyears_tf[idx]\n",
    "is_top_submission = is_top_submission[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        gc.collect()\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print('\\n memory: {} \\n'.format(process.memory_info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(maxlen,)))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "WARNING:tensorflow:From /home/david/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "12288/12800 [===========================>..] - ETA: 0s - loss: 7.5451 - acc: 0.5092\n",
      " memory: pmem(rss=647008256, vms=4368171008, shared=122826752, text=2322432, lib=0, data=917622784, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 72us/sample - loss: 7.5171 - acc: 0.5110 - val_loss: 7.5858 - val_acc: 0.5063\n",
      "Epoch 2/10\n",
      "12512/12800 [============================>.] - ETA: 0s - loss: 7.5092 - acc: 0.5113\n",
      " memory: pmem(rss=648089600, vms=4368171008, shared=122826752, text=2322432, lib=0, data=918593536, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 43us/sample - loss: 7.5192 - acc: 0.5107 - val_loss: 7.6437 - val_acc: 0.5031\n",
      "Epoch 3/10\n",
      "12416/12800 [============================>.] - ETA: 0s - loss: 7.5106 - acc: 0.5113\n",
      " memory: pmem(rss=648630272, vms=4368171008, shared=122830848, text=2322432, lib=0, data=919441408, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 43us/sample - loss: 7.5173 - acc: 0.5108 - val_loss: 7.6355 - val_acc: 0.5028\n",
      "Epoch 4/10\n",
      "12672/12800 [============================>.] - ETA: 0s - loss: 7.5160 - acc: 0.5110\n",
      " memory: pmem(rss=649711616, vms=4368171008, shared=122830848, text=2322432, lib=0, data=920317952, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 42us/sample - loss: 7.5093 - acc: 0.5115 - val_loss: 7.6045 - val_acc: 0.5053\n",
      "Epoch 5/10\n",
      "12640/12800 [============================>.] - ETA: 0s - loss: 7.4979 - acc: 0.5123\n",
      " memory: pmem(rss=650792960, vms=4368171008, shared=122830848, text=2322432, lib=0, data=921227264, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 42us/sample - loss: 7.5123 - acc: 0.5113 - val_loss: 7.5701 - val_acc: 0.5078\n",
      "Epoch 6/10\n",
      "12544/12800 [============================>.] - ETA: 0s - loss: 7.5104 - acc: 0.5112\n",
      " memory: pmem(rss=651603968, vms=4368171008, shared=122830848, text=2322432, lib=0, data=922124288, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 42us/sample - loss: 7.5092 - acc: 0.5113 - val_loss: 7.6718 - val_acc: 0.5009\n",
      "Epoch 7/10\n",
      "12032/12800 [===========================>..] - ETA: 0s - loss: 7.4850 - acc: 0.5130\n",
      " memory: pmem(rss=652414976, vms=4368171008, shared=122830848, text=2322432, lib=0, data=923037696, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 44us/sample - loss: 7.5044 - acc: 0.5118 - val_loss: 7.6380 - val_acc: 0.5034\n",
      "Epoch 8/10\n",
      "11776/12800 [==========================>...] - ETA: 0s - loss: 7.5088 - acc: 0.5115\n",
      " memory: pmem(rss=653225984, vms=4368171008, shared=122830848, text=2322432, lib=0, data=923926528, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 44us/sample - loss: 7.5055 - acc: 0.5117 - val_loss: 7.6663 - val_acc: 0.5016\n",
      "Epoch 9/10\n",
      "12096/12800 [===========================>..] - ETA: 0s - loss: 7.5424 - acc: 0.5093\n",
      " memory: pmem(rss=654307328, vms=4368171008, shared=122830848, text=2322432, lib=0, data=924844032, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 44us/sample - loss: 7.5397 - acc: 0.5095 - val_loss: 7.6291 - val_acc: 0.5031\n",
      "Epoch 10/10\n",
      "11680/12800 [==========================>...] - ETA: 0s - loss: 7.5310 - acc: 0.5099\n",
      " memory: pmem(rss=655118336, vms=4368171008, shared=122830848, text=2322432, lib=0, data=925749248, dirty=0) \n",
      "\n",
      "12800/12800 [==============================] - 1s 45us/sample - loss: 7.5220 - acc: 0.5105 - val_loss: 7.6610 - val_acc: 0.5016\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit([titles_tf, hours, dayofweeks, minutes, dayofyears_tf], [is_top_submission, is_top_submission],\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_split=split,\n",
    "#           callbacks=[MemoryCallback()])\n",
    "history = model.fit([titles_tf], [is_top_submission],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=split,\n",
    "          callbacks=[MemoryCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

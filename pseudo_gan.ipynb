{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "from keras.callbacks import TensorBoard, Callback, ModelCheckpoint\n",
    "# import tensorflow as\n",
    "# from tensorflow.keras import Input, Model, Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, concatenate,Dropout, LSTM\n",
    "# from tensorflow.keras.callbacks import TensorBoard, Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('loaded_data.pickle', 'rb') as file:\n",
    "    payload = pickle.load(file)\n",
    "\n",
    "for k in payload.keys():\n",
    "    exec('{} = payload[\"{}\"]'.format(k, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = 300\n",
    "embedding_maxfeatures = tokenizer.num_words\n",
    "embeddings_path = 'data/glove.6B.300d.txt'\n",
    "titles_maxlen = titles_train.shape[1]\n",
    "lstm_latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = {}\n",
    "weights_matrix = np.zeros((embedding_maxfeatures + 1, word_embedding_dim))\n",
    "\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if embedding_vector is not None and i <= embedding_maxfeatures:\n",
    "        weights_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_in (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_embedding (Embedding)     (None, 20, 300)      12000300    title_in[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_in (InputLayer)         (None, 20, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 570368      title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 20, 256), (N 264192      decoder_in[0][0]                 \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "main_out (Dense)                (None, 20, 40000)    10280000    decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 23,114,860\n",
      "Trainable params: 11,114,560\n",
      "Non-trainable params: 12,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "title_input = Input(shape=(titles_maxlen,), name='title_in')\n",
    "title_embedding = Embedding(\n",
    "    input_dim=tokenizer.num_words+1,\n",
    "    output_dim=word_embedding_dim,\n",
    "    weights=[weights_matrix],\n",
    "    name='title_embedding',\n",
    "    trainable=False\n",
    ")(title_input)\n",
    "\n",
    "# We don't care about the encoder sequence outputs\n",
    "_, encoder_h, encoder_c = LSTM(\n",
    "    lstm_latent_dim, return_state=True, name='encoder_lstm'\n",
    ")(title_embedding)\n",
    "\n",
    "encoder_states = [encoder_h, encoder_c]\n",
    "\n",
    "# decoder_inputs will be just zeros\n",
    "decoder_input = Input(shape=(titles_maxlen, 1), name='decoder_in')\n",
    "decoder_output, _, _ = LSTM(\n",
    "    lstm_latent_dim, return_sequences=True, return_state=True, name='decoder_lstm'\n",
    ")(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "main_out = Dense(embedding_maxfeatures, activation='softmax', name='main_out')(decoder_output)\n",
    "\n",
    "model = Model([title_input, decoder_input], main_out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'autoencoder_v0'\n",
    "log_dir = \"logs/autoencoder/scalars/{}_{}\".format(model_name,\n",
    "                                      datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "model_checkpoint_path = 'models/autoencoder/checkpoints/'+ model_name +'.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(model_checkpoint_path)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(titles, batch_size):\n",
    "    i = 0\n",
    "    while True:\n",
    "        x = titles[i:i+batch_size]\n",
    "        decoder_inputs = np.zeros((batch_size, titles_maxlen, 1))\n",
    "        y = np.zeros((batch_size, titles_maxlen, embedding_maxfeatures))\n",
    "        for j in range(batch_size):\n",
    "            for t in range(titles_maxlen):\n",
    "                y[j, t, x[j][t]] = 1\n",
    "        \n",
    "        i = (i+batch_size) % len(titles)\n",
    "        yield ([x, decoder_inputs], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3349/34942 [=>............................] - ETA: 8:04:50 - loss: 4.7886 - acc: 0.3729"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10951/34942 [========>.....................] - ETA: 6:07:56 - loss: 3.5436 - acc: 0.4985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13040/34942 [==========>...................] - ETA: 5:35:47 - loss: 3.3445 - acc: 0.5209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20667/34942 [================>.............] - ETA: 3:38:05 - loss: 2.8410 - acc: 0.5783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22738/34942 [==================>...........] - ETA: 3:06:19 - loss: 2.7422 - acc: 0.5897"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/34942 [=========================>....] - ETA: 56:53 - loss: 2.4267 - acc: 0.6265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batch(titles_train, batch_size),\n",
    "    steps_per_epoch=len(titles_train) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=generate_batch(titles_val, batch_size),\n",
    "    validation_steps=len(titles_val) // batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
